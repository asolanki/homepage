<!--
Autogenerated comment from sync_posts.py
Title: Journey to LLMs: The Roots of NLP
Date: 2023-09-03
Description: Exploring the history and foundations of natural language processing that paved the way for modern language models.
-->
<!DOCTYPE html>
<html lang="en" data-theme="dark">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- POST TITLE -->
  <title>
    Journey to LLMs: The Roots of NLP
  </title>

  <!-- POST META -->
  <meta name="title" content="Journey to LLMs: The Roots of NLP">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:description" content="Exploring the history and foundations of natural language processing that paved the way for modern language models.">
  <meta name="twitter:title" content="Journey to LLMs: The Roots of NLP">
  <link rel="stylesheet" href="../css/pico.min.css">
  <link rel="stylesheet" href="../css/styles.css">
  <link rel="icon" type="image/png" href="../img/favicon.png">
</head>

<body>
  <main class="container">

    <nav>
      <ul>
        <li><a href="../index.html" class="secondary"><small>Adarsh Solanki</small></a></li>
      </ul>

      <ul>
        <li><a href="../posts.html" class="secondary"><small>Blog</small></a></li>
        <li><a href="../about.html" class="secondary"><small>About Me</small></a></li>
        <li><a href="#" id="toggle-dark" data-tooltip="Toggle Dark"><svg width="18" height="18" viewBox="0 4 24 18"
              preserveAspectRatio="xMidYMid meet" xmlns="http://www.w3.org/2000/svg">
              <path fill-rule="evenodd" clip-rule="evenodd"
                d="M12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                fill="currentColor" />
              <path fill-rule="evenodd" clip-rule="evenodd"
                d="M11 0H13V4.06189C12.6724 4.02104 12.3387 4 12 4C11.6613 4 11.3276 4.02104 11 4.06189V0ZM7.0943 5.68018L4.22173 2.80761L2.80752 4.22183L5.6801 7.09441C6.09071 6.56618 6.56608 6.0908 7.0943 5.68018ZM4.06189 11H0V13H4.06189C4.02104 12.6724 4 12.3387 4 12C4 11.6613 4.02104 11.3276 4.06189 11ZM5.6801 16.9056L2.80751 19.7782L4.22173 21.1924L7.0943 18.3198C6.56608 17.9092 6.09071 17.4338 5.6801 16.9056ZM11 19.9381V24H13V19.9381C12.6724 19.979 12.3387 20 12 20C11.6613 20 11.3276 19.979 11 19.9381ZM16.9056 18.3199L19.7781 21.1924L21.1923 19.7782L18.3198 16.9057C17.9092 17.4339 17.4338 17.9093 16.9056 18.3199ZM19.9381 13H24V11H19.9381C19.979 11.3276 20 11.6613 20 12C20 12.3387 19.979 12.6724 19.9381 13ZM18.3198 7.0943L21.1923 4.22183L19.7781 2.80762L16.9056 5.6801C17.4338 6.09071 17.9092 6.56608 18.3198 7.0943Z"
                fill="currentColor" />
            </svg></a></li>
      </ul>
    </nav>

    <!-- POST TITLE -->
    <nav aria-label="breadcrumb">
      <small>
        <ul>
          <li>
            <a href="../posts.html">All Posts</a>
          </li>
          <li>
            Journey to LLMs: The Roots of NLP
          </li>
        </ul>
      </small>
    </nav>

    <!-- POST CONTENT -->
    <div id="post-content">

      <!-- hero image -->
      <div class="center">
        <img src="../img/nlp/nlp1.webp" alt="Early Human Machine Interaction">
      </div>

      <!-- POST TITLE -->
      <h3>Journey to LLMs: The Roots of NLP</h3>

      <p>With the explosive rise of Large Language Models (LLMs), most notably the
        <a
          href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">
          viral explosion of ChatGPT
        </a>, I joined many others in developing a fascination with how these systems work.
      </p>

      <p>How did we reach a stage in natural language processing (NLP) where by training incredibly large models with
        billions (or even <a
          href="https://www.notion.so/Journey-to-LLMs-The-Roots-of-NLP-7f3e32949bd645cb9f7e32be2d60ed75?pvs=21">
          trillions</a>) of parameters models begin to show <a href="https://arxiv.org/abs/2206.07682">emergent
          behaviors</a>
        that eerily mimic human language understanding, even though their basic function is simply to predict the next
        token
        in a string of text?
      </p>

      <p>I wanted to understand the basics of NLP, as well as the history leading to these recent advancements.
        My hope is that this series of posts can serve as notes for my own learning, as well as a resource to help
        others looking to learn more about NLP.
      </p>

      <h4>Can we talk to machines?</h4>

      <p>NLP aims to solve a simple question: how do we bridge the gap between machines and humans? Just as we
        strive to communicate with fellow humans and other living species, we’ve long sought to enhance our ability
        to “speak” to the machines we build.
      </p>

      <p>At first, we adapted our communication to the machines themselves, but over time we were able to build
        increasingly human-centered communication.
      </p>

      <h4>The Turing Test: Can Machines Think?</h4>

      <p>The question of whether or not a computational machine can “think” or communicate at a human level has
        been a topic of debate and fascination in the world of computation since the very beginning. This idea is
        the foundation of the <a href="https://en.wikipedia.org/wiki/Turing_test">Turing Test</a>, proposed by the
        renowned mathematician and computer scientist Alan Turing in his groundbreaking paper “Computing Machinery
        and Intelligence” in 1950. (
        <a
          href="https://watermark.silverchair.com/lix-236-433.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA1swggNXBgkqhkiG9w0BBwagggNIMIIDRAIBADCCAz0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMuvx2n3IKNYZ6-2rxAgEQgIIDDrunjs6Q3uKksJk_nt3-Ur6tlBSpjQD4_pWkpJ5XDYixGKr7tqVdYPjlpyq0I4qvPMSK40BGUB6V4X49Hc8pT0eVcwrd9ZsQNpeU4jVU6H5xACWtJ3MWsoCDZjf7DFxB9Y4I_obEAWor81eToD5g4vsEmQAXubEE20EOuXJfC7Ga_1ha6qXOrB7MyEO15jFlgb6bDJvw90HAd0DtZiOexYaHcA6bUvB72OHzcG8wHUNWXKbIWPrCA1-ARyBbVFg59RL4QUnK9inb02n0r4LS8aZF_9RA-Kwhwl69DC52v4mRuVBQPcN6ANvzj_5L6-paar6AVPZaECZ_tVnMTy4iwrNOKyx6VEURN15HcXXqxmyaL24XBap79nJorwTf8jb487JTiL4Q4cWsYIhAYeWuS6nz6FGoQldqmVkLo7MxADeQO2b09JMYvHzGZXF1K74R6uq0Y-czRYUoSJ6ADxsISxQ3uaUcgUR4ulGs-9ZCdpwc9KhkkzwC8JuREEP5GUEj9nu54JHv5Z-r9dFm6ihd1g8V7SNooJPtBAsiqXbOB88KRN-gXZz0vOISeoSR9RxaJ4LEsR6mbc56TGi48QTebQuRHbN2ow3IS_jQF-z5qAniXCebVfzrMr3JbgXFRM6d7WuDg6hZwdXGfQBVQjeePHabmN-Fb0rrjHhgo5WDmb1X0LgSds01799JbAOfLeLzMRScvxSk22a_t0JPJ6QV5vJq72lXYLCK0c0-YaFY1EZ4dwDZn2vxIop_hX1b5Ni6AHSqE9NspRRqddsCbg_eEyMKPCjGmhA19WCg30sLC73z5f0Fo8Ui2QV7y6LKfcRnFJpj23OKYeq1iMfBMI9pM-J-gr7NND9f28iST2L68Hqx8JD-LeSs2bss_464zsyxmpdJFD0uHFeRj7ciuzg-gubSQC7VpLCnXnxlSQg6Qiz-4M1VC056x-4Xzrl6NxTsuXgHGTX2VIwQ_YkFvUYJFC1cSheltnvO_UQnyVDtLo0iyNmc-MhreJ202GZEaumDzZOt-vpQK74qTcqNN2u8">full
          text PDF</a>,
        <a href="https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence">wikipedia</a>).
      </p>

      <p>The Turing Test, or Imitation Game as it was originally sometimes called, is designed to measure a machine's
        ability to exhibit intelligent behavior that is indistinguishable from that of a human being. In the test, a
        human evaluator interacts with an unseen party, and the evaluator's task is to determine whether they are
        talking to a human or a machine based on the conversation. If the evaluator cannot reliably distinguish the
        machine from the human, the machine is said to have passed the test.
      </p>

      <div class="center">
        <img src="../img/nlp/turingtest.gif" alt="Turing Test" style="width: 400px !important">
      </div>


      <p>The Turing Test established a north-star benchmark for assessing just how closely machines can mimic
        human-like conversational abilities.
      </p>

      <p>While some critics argue that the test is not a sufficient measure of all types of human intelligence
        (such as emotional intelligence), it nonetheless provides a practical way to gauge a machine's ability
        to mimic human-like responses.
      </p>

      <p>However, this level of sophisticated interaction didn't appear overnight. To appreciate the advances
        we've made in natural language processing, it's helpful to take a step back and examine how humans
        initially began to communicate with machines at a far more basic level.
      </p>

      <h4>Early Machine Interactions: Punch Cards</h4>

      <p>One of the earliest forms of this rudimentary communication is the 18th century invention of punch card looms
        like the <a href="https://en.wikipedia.org/wiki/Jacquard_machine">Jacquard machine</a>. These machines used
        "punched cards" to control weaving patterns, offering a simple but effective way to program mechanical
        operations.
      </p>

      <div class="center">
        <img src="../img/nlp/jacquardloom.png" alt="Jacquard loom punch card (~1800)" width="500px">
        <p class="sub">Jacquard loom punch card (~1800)</p>
      </div>

      <p>Punched cards carried forward into the next century, notably in Charles Babbage’s theoretical
        <a href="https://en.wikipedia.org/wiki/Difference_engine">Difference Engine</a>, considered the first
        mechanical computer design, and were popularized with the rise of the “<a
          href="https://www.ibm.com/ibm/history/ibm100/us/en/icons/punchcard/">IBM card</a>” used by early
        data processing machines.
      </p>

      <div class="center">
        <img src="../img/nlp/ibmcard.png" alt="The 80-column IBM card (1928) " width="500px">
        <p class="sub">The 80-column IBM card (1928)</p>
      </div>

      <p>Machine interaction continued to evolve towards a more human-centered experience as a result of the following
        two key developments:
      </p>

      <h6>Machine Code to High-Level Programming Languages</h6>

      <p>The progression from binary machine code to higher-level languages like FORTRAN (1954) and COBOL (1959) was
        transformative. They added layers of abstraction closer to human language and thought.
      </p>

      <p>Here are some examples demonstrating code adding “2 + 3” across the evolution of human-machine interaction,
        from x86 machine code, to assembler, to FORTRAN, to ChatGPT:
      </p>

      <div class="gist-container">
        <script src="https://gist.github.com/asolanki/bb2da694ce5f865989509ea94acd574c.js"></script>


        <script src="https://gist.github.com/asolanki/39d643d1a71622f4ebc0a9a8e7ece987.js"></script>

        <script src="https://gist.github.com/asolanki/4a5895881d3abdac37889393944ba8c6.js"></script>
      </div>


      <div class="center">
        <img src="../img/nlp/chatgpt.png" alt="ChatGPT solving 2+3" style="width:450px !important">
      </div>

      <h6>Punch Cards to Text Terminals</h6>

      <p>
        The meticulous punch card programming workflow slow and error prone. Each card held one line of code,
        and a stack of these cards represented a full program. Even the process of feeding these cards into a machine
        was cumbersome, and a single error could mean starting the whole process over again.
      </p>

      <p>
        The introduction of terminals in the 1960s marked a massive change in this process, allowing programmers to
        write,
        edit, and execute code directly, no longer constrained to their stacks of paper cards.
      </p>

      <p>
        This transition moved programming closer towards the way we already used machines to talk to humans - typing on
        a
        typewriter. The shift to terminals brought a more intuitive and human-friendly way of interacting with machines
      </p>

      <h3>ELIZA and Rule-based NLP Systems</h3>

      <p>
        The first systems resembling modern NLP used rule-based approaches. Programmers manually coded predefined rules
        for processing language.
      </p>

      <p>
        One of the most well-known was <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a>, created by Joseph
        Weizenbaum
        at MIT in the 1960s. ELIZA simulated a Rogerian psychotherapist using pattern matching on key words. It was an
        early
        “ChatGPT moment” that convincingly simulated human conversation, despite its simplicity. Many early users were
        convinced
        that ELIZA possessed a true sense of intelligence, perhaps even passing the Turing test.
      </p>

      <div class="center">
        <img src="../img/nlp/eliza.png" alt="Welcome screen for Eliza" width="500px">
      </div>

      <p>
        While innovative, ELIZA had no real language understanding - it followed programmed response rules. Here is a
        snippet
        of ELIZA’s logic in simplifed Python:
      </p>

      <div class="gist-container">
        <script src="https://gist.github.com/asolanki/f70394ae96a3a89fbac6253e08f836c2.js"></script>
      </div>

      <p>Rules-based systems demonstrated some great early results, but had clear limitations: </p>
      <ul>
        <li>
          <b>Labor-intensive</b>: Manually creating rules is a lot of work. In the above example, one would have to
          enumerate all of the possible words that could relate to the “family” response, including possibly typos.
        </li>

        <li>
          <b>Can’t handle ambiguity</b>: There is more to responding to a person than pattern matching on a specific
          word or phrase. For example, if the input is “I am sick of talking about my mother”, the simple system above
          would still respond with “Tell me more about your family,” leading to an unsatisfying experience.
        </li>

        <li>
          <b>Inherently static</b>: Rule-based systems of this era could not learn or adapt without direct updates to
          the rules in the source code. As a result, the responses can quickly grow stale and repetitive, even if
          multiple
          options are available for each rule.
        </li>
      </ul>

      <p>
        Luckily, developments in the field of linguistics led to new approaches taking advantage of a deeper
        understanding of the
        structure and syntax of natural language.
      </p>

      <h3>Chomsky’s Grammars and the Earley Parser</h3>

      <p>
        Though perhaps more commonly known now for his social critiques and status as a public intellectual, Noam
        Chomsky’s
        largest impact was in his primary discipline of linguistics. This “mathematical” approach to grammar was
        credited by
        computer science legends like <a href="https://en.wikipedia.org/wiki/Donald_Knuth">Donald Knuth</a> and
        <a href="https://en.wikipedia.org/wiki/John_Backus">John Backus</a> as a major influence.
      </p>

      <p>
        Chomsky’s <a href="https://en.wikipedia.org/wiki/Linguistics_of_Noam_Chomsky">theories of generative grammar</a>
        laid the groundwork for the era of syntactic analysis in NLP was born.
        This approach sought to understand and represent the structure and rules of language, taking into account the
        ways words
        are combined to form phrases, clauses, and sentences.
      </p>

      <p>
        Chomsky developed a <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">hierarchy of grammars</a> or
        language rules
        that categorized different languages by complexity laid the foundation for algorithmic parsing of natural
        language based
        on syntactic structures. It defined four levels of grammars with increasing complexity, and while a detailed
        dive into
        these grammars is out of scope for this post, it is a rich and relevant subject if you have an interest in
        theoretical
        computer science.
      </p>

      <p>
        One of the first practical applications of Chomsky’s theories was the Earley parser, developed by Jay Earley at
        Stanford in his doctoral thesis <a href="https://dl.acm.org/doi/10.1145/362007.362035">"An Efficient
          Context-Free Parsing Algorithm."</a>
        The parsing algorithm was designed to handle “context-free” or “Type-2” grammars as defined in Chomsky’s
        hierarchy,
        which were complex enough to parse natural language sentences efficiently, marking a significant stride in the
        field.
      </p>

      <p>
        This shift towards syntactic analysis and parsing led to the development of more sophisticated NLP systems which
        could
        understand sentence structure and the relationships between words, enabling more complex and ambiguous inputs.
      </p>

      <p>While these systems were a significant improvement over rule-based systems, they still had limitations:</p>

      <ul>
        <li>
          <b>Limited semantic understanding</b>: While they could handle the structure of sentences, syntactic systems
          often struggled with understanding the meanings of words and phrases in context. For example, in the sentence
          “The man saw the woman with the telescope” does the man or the woman have the telescope?
        </li>

        <li>
          <b>Difficulty with idiomatic expressions</b>: These systems often struggled with idioms and other non-literal
          uses of language. For example, to understand a phrase such as “kicking the bucket,” it takes specific cultural
          context, not just an understanding of the English language.
        </li>

        <li>
          <b>Still static</b>: Like rule-based systems, syntax-based systems were static and could not learn or adapt to
          new inputs without being explicitly programmed to do so, similar to a manual that needs constant updates.
        </li>
      </ul>

      <p>
        Despite these limitations, the era of syntactical analysis was a crucial step forward in the development of NLP
        that could somewhat “understand” natural language. This era laid the groundwork for the statistical and machine
        learning approaches that would come next, and many of the concepts and techniques developed during this era are
        still used in modern NLP systems.
      </p>

      <h3>Conclusions</h3>

      <p>
        From the rule-based systems of ELIZA to the syntactic analysis of the Earley parser, each step has brought us
        closer to the goal of creating machines that can truly understand and generate human language. While it can be
        exciting to focus on the most cutting-edge developments, it has been great to learn about the decades of
        foundational research that made it all possible. Appreciating this history provides perspective on just how far
        NLP has advanced.
      </p>

      <p>
        As someone without extensive background in NLP, understanding these pioneering systems provides very helpful
        context in understanding later systems. Though today the story of NLP focuses on the technological advancements
        of machine learning model architectures and GPUs, modern NLP is actually part longer tale of human ingenuity and
        curiosity in the pursuit of understanding one of the most fundamental elements of the human experience:
        language. Every leap in NLP was built on the achievements and lessons of the past, with step-wise increments
        following each significant innovation.
      </p>

      <p>
        While these early systems may seem rudimentary now, I remain impressed by what early researchers accomplished
        with limited tools and prior work to learn from. Trying basic examples myself, I gained appreciation for the
        immense effort required to manually craft a language model. Developing a chatbot like ELIZA, even with
        contemporary tooling, would be remarkably laborious.
      </p>

      <p>
        In the next post, we’ll explore the rise of statistical techniques and machine learning in NLP, establishing the
        next set of building blocks towards today’s state-of-the-art LLMs. Stay tuned!
      </p>


      <h3>Appendix</h3>

      <ul>
        <li>
          <a href="https://dl.acm.org/doi/10.1145/365153.365168">”ELIZA—a computer program for the study of natural
            language communication between man and machine”</a> by Joseph Weizenbaum, 1966
        </li>
        <li>
          <a href="https://github.com/jeffshrager/elizagen.org/tree/master/1965_Weizenbaum_MAD-SLIP">The annotated ELIZA
            source code on Github</a>
        </li>
      </ul>

      <div class="center">
        <img src="" alt="" width="500px">
        <p class="sub"></p>
      </div>
    </div>
  </main>

  <footer>
    <div class="container">
      <small>
        <p>© 2023 Adarsh Solanki</p>
      </small>
    </div>
  </footer>
</body>

<script>
  // toggle dark mode
  document.getElementById('toggle-dark').addEventListener('click', function () {
    var currentTheme = document.documentElement.getAttribute('data-theme');
    if (currentTheme === 'light') {
      document.documentElement.setAttribute('data-theme', 'dark');
    } else {
      document.documentElement.setAttribute('data-theme', 'light');
    }
  });
</script>

</html>